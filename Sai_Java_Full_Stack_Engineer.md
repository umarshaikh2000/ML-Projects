PROFESSIONAL EXPERIENCE
**STRONG AREAS:**
Java 8+ | Spring Boot | RESTful APIs | PostgreSQL | Database Design | ETL | Business Analysis | Microservices | Performance Optimization | Requirements Gathering | Data Modeling | Agile/Scrum

Currently working with client Signify as Full Stack Java Engineer for Trillix - Enterprise Workflow & Data Management System

**Major Tools, Associated Technologies & Environment**

**SAI**
Java Full Stack Engineer @
Intactbox Technologies

Database Used - PostgreSQL, pgAdmin, DBeaver
Backend Framework: Spring Boot, Spring MVC, Spring Data JPA, Hibernate
Data Processing: Spring Batch, ETL Pipelines
Frontend Technology - React, HTML, CSS, JavaScript
API Development: RESTful Web Services
Build Tool: Maven, Git
Documentation Tools: Confluence, JIRA, Draw.io

**CAREER OBJECTIVE**

**Technical Qualification Headlines**

To employ myself in a progressive
organization that provides scope to update
my knowledge and skills in accordance with
the latest trends and be part of the team
that dynamically works towards growth of
the organization.

Spring Boot | Java | RESTful APIs | PostgreSQL | Database Design | Data Engineering | ETL | Business
Analysis | Microservices | Spring Data JPA | Hibernate | Performance Optimization | Requirements
Analysis | Agile/Scrum | CI/CD |

**Associated Technology:** AWS, Docker, Git, Maven, Postman, IntelliJ IDEA, Jenkins, JUnit, Mockito,
Flyway, Liquibase, Spring Batch, Apache POI

Looking for a challenging Java Engineering
position where I can utilize my full stack
development, data engineering, and
business analysis skills to build scalable
enterprise applications and drive business
value.

**Work Experience with clients:** Signify (Netherlands/Global) | Previous Client Experience

**System/Vertical Worked with:** Enterprise Workflow Management | Data Warehousing | Business
Process Automation | Payment Analytics | User Management Systems | ETL Pipelines

To achieve a challenging position as a Java
Full Stack Engineer in a result-oriented
company, where acquired skills and
education will be utilized towards
continuous growth and advancement.

**Experience with Core Java/Spring Libraries/Packages:**

Spring-boot-starter-web | Spring-boot-starter-data-jpa | Spring-boot-starter-security | Spring-batch |
Hibernate-core | Jackson | Lombok | Slf4j | JUnit | Mockito | Spring-boot-starter-validation | HikariCP
| Flyway | Apache POI | QueryDSL

**Experience with Database & Data Technologies:**

PostgreSQL (Advanced) | Database Schema Design | Normalization | Indexing | Query Optimization |
Stored Procedures | Triggers | Views | Partitioning | ETL | Data Migration | Data Warehousing | JDBC

**Experience with Business Analysis Tools:**

JIRA | Confluence | Draw.io | MS Visio | Lucidchart | Postman | Excel (Advanced) | PowerPoint | UML |
BPMN | Process Modeling

**Process worked with:**

DevOps + Agile/Scrum + CI/CD Pipeline + Requirements Analysis + Data Quality Assurance

**+91-XXXXXXXXXX**
**<sai@email.com>**
**Location, India**
**English | Hindi | [Your Language]**

**EDUCATION**
**Degree:** [Your Degree]
**University:** [Your University]
**Graduation:** [Year]

**KEY SKILLS**
**Frameworks:** Spring Boot, Spring MVC, Spring Data JPA, Hibernate ORM, Spring Security, Spring Batch
**Web Technologies:** RESTful APIs, JSON, XML, HTTP/HTTPS, React (Integration)
**Programming Languages:** Java 8+ (Advanced), SQL (Advanced), PL/pgSQL
**Version Control:** Git, GitHub, GitLab
**Cloud Computing:** AWS (EC2, S3, RDS), Docker, Kubernetes (Basic)
**Data Technologies:** ETL, Data Modeling, Data Warehousing, Data Migration, Business Intelligence
**Build Tools:** Maven, Gradle
**Application Servers:** Apache Tomcat, Nginx
**Databases:** PostgreSQL (Expert), MySQL, Oracle, MongoDB
**IDEs/Development Tools:** IntelliJ IDEA, Eclipse, Postman, DBeaver, DataGrip, pgAdmin
**Operating Systems:** Windows, Linux (Ubuntu, CentOS, Red Hat)
**Protocols:** TCP/IP, HTTP/HTTPS, REST, SOAP
**Deployment Tools:** Docker, Jenkins, CI/CD Pipelines
**Issue Trackers:** JIRA, Confluence
**Web Service:** REST-JSON, Spring REST
**Business Analysis:** Requirements Gathering, Gap Analysis, Process Modeling, User Story Creation, Stakeholder Management
**Modeling Tools:** UML, BPMN, Draw.io, MS Visio, Lucidchart
**Data Tools:** Flyway, Liquibase, Apache POI, CSV Processing, Excel, Tableau (Basic)
**Testing:** JUnit, Mockito, Integration Testing, UAT

---

## Working Zone Organization

**Currently working as Java Full Stack Engineer with Intactbox Technologies - December 2024 – Present (6 months)**

**Previously worked as Java Developer with Inbatoz Solutions - May 2023 – September 2024 (17 months)**

---

## Projects

### **Project Sequence 1**

**Project Name:** Trillix – Enterprise Workflow & Data Management Platform
**Vertical:** Business Process Automation, Data Engineering & Analytics
**Client:** Signify (Netherlands/Global)
**Technology & Tool:** Java, Spring Boot, Spring Data JPA, PostgreSQL, Hibernate, Spring Batch, ETL, REST APIs, React, Maven, Git, Postman, Docker, JIRA, Confluence
**Team Size:** 11
**Roles:** Full Stack Java Engineer (Backend Developer, Data Engineer, Business Analyst)

**Detail Project Overview and Workflow:**

Trillix is a comprehensive enterprise-grade workflow and data management platform developed for Signify, a global leader in lighting and IoT solutions. The system serves as the backbone for managing complex business processes, user workflows, data operations, and analytics across multiple departments and geographical locations. The platform processes millions of transactions daily, managing data from various sources while providing real-time visibility into organizational operations.

As a **Full Stack Java Engineer**, the role encompassed three critical dimensions: **backend development, data engineering, and business analysis**. This multifaceted responsibility required bridging the gap between business stakeholders and technical implementation, designing robust database architectures, and developing high-performance backend services.

**Business Analysis & Requirements Engineering:**

The project commenced with extensive **requirements gathering** from Signify's stakeholders across operations, finance, HR, and management departments. Through structured interviews, workshops, and documentation reviews, critical business pain points were identified including manual workflow processing, lack of real-time visibility, data silos, inefficient approval processes, and reporting limitations.

Business requirements were analyzed and translated into technical specifications using **user stories** in JIRA with clear acceptance criteria, story points, and business value prioritization. Each requirement was traced through design, implementation, testing, and deployment phases ensuring complete coverage.

**Process modeling** was performed using **BPMN (Business Process Model and Notation)** diagrams in Draw.io, visualizing current "as-is" processes and proposed "to-be" automated workflows. These process flows helped stakeholders understand the transformation, identify bottlenecks, and validate solutions before development. **Gap analysis** compared current capabilities with desired functionality, identifying technical constraints and integration requirements.

**Stakeholder management** involved regular communication with Signify's product owners, business users, and executives through requirement review sessions, sprint demos, UAT coordination, and status updates. Conflicting requirements were resolved through facilitated discussions ensuring alignment on priorities and scope.

**Backend Development & API Architecture:**

The backend architecture was built using **Spring Boot** framework following clean **layered architecture** with distinct controller, service, and repository layers. This modular design ensured separation of concerns, maintainability, testability, and scalability of the codebase.

**RESTful APIs** were designed and developed providing comprehensive functionality for:

- **User Management**: Registration, authentication, authorization, profile management, password reset
- **Workflow Operations**: Create, execute, approve, reject, delegate workflows
- **Task Management**: Assignment, tracking, notifications, escalations
- **Reporting & Analytics**: Dashboards, custom reports, data export
- **Administrative Functions**: Configuration, user roles, permissions, audit logs

All APIs followed **REST best practices** with proper HTTP methods (GET, POST, PUT, DELETE), status codes (200, 201, 400, 401, 403, 404, 500), proper error responses, and pagination for large datasets. **API documentation** was created using Swagger/OpenAPI providing interactive documentation for frontend developers and QA teams.

**Security** implementation using **Spring Security** included:

- JWT-based authentication for stateless API access
- Role-based access control (RBAC) with granular permissions
- Password encryption using BCrypt algorithm
- CSRF protection and SQL injection prevention
- Input validation and sanitization
- Rate limiting and DDoS protection

The **performance optimization** strategy included:

- Redis caching for frequently accessed data (40% reduction in database load)
- Database query optimization reducing response times by 40%
- Connection pooling using HikariCP with optimal pool sizing
- Asynchronous processing for long-running tasks using @Async
- Pagination and lazy loading for large datasets
- Database indexing strategies for read-heavy operations

**Data Engineering & Database Architecture:**

The **PostgreSQL database** formed the foundation with a meticulously designed schema consisting of **50+ normalized tables** supporting complex business relationships. Database design followed **Third Normal Form (3NF)** principles eliminating redundancy while maintaining referential integrity through foreign key constraints, triggers, and stored procedures.

The schema supported critical features including:

- Multi-tenancy architecture for organizational hierarchies
- Temporal data tracking with effective dating
- Comprehensive audit logging for compliance
- Hierarchical structures for workflows and approvals
- Complex relationships between users, roles, workflows, tasks

**Database optimization** was a continuous focus area:

- Comprehensive **indexing strategies** including B-tree indexes, partial indexes, composite indexes
- Query performance analysis using **EXPLAIN ANALYZE** identifying bottlenecks
- Query rewrites and optimization improving execution times by 40%
- **Partitioning strategies** for large tables improving query and maintenance performance
- **Materialized views** for complex analytical queries reducing computation overhead
- Regular maintenance tasks (VACUUM, ANALYZE, REINDEX) maintaining optimal performance

**Data access layer** implementation using **Spring Data JPA** and **Hibernate ORM**:

- Custom repository methods using JPQL and Criteria API for complex queries
- Native SQL for performance-critical operations
- Second-level caching using Ehcache reducing database hits by 30%
- Batch operations for bulk inserts/updates
- Transaction management ensuring ACID properties
- Optimistic locking for concurrent data access

**ETL Pipelines & Data Integration:**

**Spring Batch** framework was used to develop comprehensive ETL pipelines handling:

- **Data Import**: CSV files, Excel spreadsheets, external APIs, legacy systems
- **Data Transformation**: Cleansing, validation, enrichment, normalization
- **Data Loading**: Bulk inserts, upserts, error handling
- **Data Export**: Reports in CSV, Excel, PDF formats using Apache POI and JasperReports

ETL jobs included:

- Chunk-based processing for memory efficiency
- Transaction management with commit intervals
- Error handling and retry mechanisms
- Job restart capabilities for failed runs
- Comprehensive logging and monitoring
- Scheduling using Quartz or Spring Task Scheduler

**Data migration strategies** using **Flyway** for version-controlled database changes:

- Versioned migration scripts for schema evolution
- Repeatable migrations for views and procedures
- Testing in staging environments before production
- Rollback strategies for major changes
- Zero-downtime deployment approaches

**Data Quality & Governance:**

- Automated **data validation framework** catching 95% of anomalies before persistence
- Business rule implementation ensuring data integrity
- Data profiling and quality metrics tracking
- Reconciliation reports for data accuracy verification
- Comprehensive **audit logging** tracking all data modifications (INSERT, UPDATE, DELETE) with user, timestamp, and change details

**Frontend Integration & Collaboration:**

Integration with **React frontend** required:

- Defining clear API contracts and request/response formats
- Error handling conventions and status code standards
- Authentication mechanisms (JWT token flow)
- WebSocket integration for real-time notifications
- CORS configuration for cross-origin requests
- API versioning strategies for backward compatibility

Close collaboration with frontend developers ensured seamless user experience with proper loading states, error messages, and data synchronization.

**Development Practices & Team Collaboration:**

**Agile/Scrum** methodology with active participation in:

- **Sprint Planning**: Breaking user stories into tasks, estimation, capacity planning
- **Daily Standups**: Progress updates, blocker identification, team coordination
- **Sprint Reviews**: Feature demonstrations to stakeholders, feedback gathering
- **Retrospectives**: Process improvement discussions, team dynamics
- **Backlog Refinement**: Requirement elaboration, acceptance criteria definition

**Code quality** maintained through:

- Comprehensive **code reviews** ensuring standards, best practices, security, performance
- **Unit testing** using JUnit and Mockito with 85%+ code coverage
- **Integration testing** for API endpoints and database operations
- Static code analysis using SonarQube
- Continuous integration using Jenkins with automated test execution

**Technical Documentation:**

- API specifications with Swagger/OpenAPI
- Database schema documentation with ER diagrams
- System architecture diagrams
- Deployment and configuration guides
- User manuals and training materials
- Requirement traceability matrices

**Deployment & DevOps:**

- **Docker containerization** for consistent environments
- **CI/CD pipelines** using Jenkins for automated builds, tests, and deployments
- Environment-specific configurations (dev, staging, production)
- Blue-green deployment strategies for zero downtime
- Monitoring and alerting using application logs and metrics

**Impact & Business Value:**

The system delivered significant business value:

- **Workflow automation** reducing manual processing time by 60%
- **Data accuracy** improvement through validation and business rules
- **Real-time visibility** into operations enabling faster decision-making
- **40% API performance improvement** through optimization efforts
- **30% database load reduction** through caching strategies
- **Enhanced user experience** through intuitive interfaces and responsive APIs
- **Compliance** through comprehensive audit logging
- **Scalability** supporting growing user base and transaction volumes

**Task Handled:**

**Business Analysis & Requirements:**

- Conducted **stakeholder interviews** with Signify business users across multiple departments identifying pain points
- Facilitated **requirements gathering workshops** for operations, finance, HR teams
- Created **user stories in JIRA** with clear acceptance criteria, story points, and business value
- Developed **BPMN process flow diagrams** modeling current and future state workflows
- Performed **gap analysis** comparing manual processes with proposed automation
- Created **functional requirement specifications** documenting system capabilities
- Conducted **feasibility analysis** evaluating technical viability, resources, timelines
- Managed **stakeholder expectations** through regular communication and demos
- Prioritized **product backlog** based on business value, dependencies, ROI
- Resolved **requirement conflicts** through facilitated discussions and consensus
- Created **use case diagrams** illustrating system interactions and scenarios
- Documented **business rules** and validation logic for implementation
- Developed **wireframes and mockups** for UI/UX validation
- Coordinated **user acceptance testing** (UAT) with business users
- Created **training materials** and user guides for system adoption
- Gathered **user feedback** during sprint demos and incorporated into backlog

**Backend Development & API:**

- Designed and developed **RESTful APIs** using Spring Boot for workflow, user management, reporting
- Implemented **layered architecture** (controller-service-repository) ensuring separation of concerns
- Developed **user authentication** using Spring Security with JWT tokens
- Implemented **role-based access control** (RBAC) with granular permissions
- Created **comprehensive error handling** with proper HTTP status codes and messages
- Developed **input validation** framework using Bean Validation and custom validators
- Implemented **pagination, sorting, filtering** for efficient data retrieval
- Created **audit logging system** tracking user actions for compliance
- Optimized **API response times** from 200ms to 120ms through various techniques
- Implemented **caching strategies** using Redis reducing database load by 30%
- Developed **asynchronous processing** for long-running tasks using @Async
- Created **batch APIs** for bulk operations improving efficiency
- Implemented **transaction management** using @Transactional for data consistency
- Developed **custom exception handling** with proper error responses
- Created **API documentation** using Swagger/OpenAPI for team reference
- Implemented **API versioning** for backward compatibility
- Developed **WebSocket endpoints** for real-time notifications
- Configured **CORS settings** for frontend integration
- Implemented **rate limiting** for API protection

**Data Engineering & Database:**

- Designed **PostgreSQL database schema** with 50+ normalized tables
- Implemented **indexing strategies** (B-tree, partial, composite) improving query performance by 40%
- Developed **data access layer** using Spring Data JPA with custom repository methods
- Created **complex SQL queries** with joins, subqueries, CTEs, window functions
- Implemented **stored procedures and triggers** for business logic and data integrity
- Optimized **database queries** using EXPLAIN ANALYZE and rewrites
- Configured **connection pooling** (HikariCP) for optimal resource utilization
- Implemented **second-level caching** (Ehcache) reducing database hits
- Developed **partitioning strategies** for large tables
- Created **materialized views** for analytical queries
- Implemented **ETL pipelines** using Spring Batch for data import/export/transformation
- Designed **data migration scripts** using Flyway for version-controlled changes
- Developed **data validation framework** catching 95% of anomalies
- Created **audit tables** tracking all data modifications
- Implemented **bulk data operations** processing thousands of records efficiently
- Developed **data export functionality** (CSV, Excel, PDF) using Apache POI
- Created **database monitoring queries** for performance tracking
- Implemented **backup and recovery strategies**
- Developed **data profiling reports** for quality assurance
- Created **database documentation** with ER diagrams and data dictionary

**Testing & Quality Assurance:**

- Developed **unit tests** using JUnit achieving 85%+ code coverage
- Created **integration tests** for API endpoints and database operations
- Implemented **mock testing** using Mockito for service layer
- Conducted **performance testing** identifying bottlenecks
- Participated in **UAT coordination** with business users
- Fixed **bugs and defects** identified during testing phases

**DevOps & Deployment:**

- Created **Docker containers** for application deployment
- Configured **CI/CD pipelines** using Jenkins for automated builds and deployments
- Managed **environment configurations** (dev, staging, production)
- Implemented **logging** using Slf4j and Logback
- Configured **monitoring and alerting** for production environments
- Performed **production deployments** with zero downtime

**Collaboration & Communication:**

- Participated in **Agile ceremonies** (planning, standups, reviews, retrospectives)
- Conducted **code reviews** ensuring quality, standards, best practices
- Collaborated with **frontend team** for seamless API integration
- Mentored **junior developers** on Spring Boot and best practices
- Created **technical documentation** for architecture, APIs, database
- Facilitated **knowledge sharing sessions** within the team
- Presented **technical solutions** to stakeholders and management

---

### **Project Sequence 2**

**Project Name:** Smart Maintenance Solution - Industrial Automation System
**Vertical:** Industrial Automation & IoT
**Client:** Elsner Engineering, USA
**Technology & Tool:** Java, Spring Boot, Spring Data JPA, MySQL, REST APIs, Apache Kafka, MQTT, Redis, Docker, AWS IoT Core, Maven, Git
**Team Size:** 8
**Duration:** May 2023 - December 2023 (8 months)
**Role:** Java Backend Developer

**Detail Project Overview and Workflow:**

Elsner Engineering specializes in delivering purpose-built industrial machinery tailored to specific client needs. A notable product is the ENR-G3-48 Rewinding Machine, which operates at speeds up to 800 feet per minute, producing 128 rolls per minute of standard canister wipes. The Smart Maintenance Solution project aimed to optimize industrial automation systems by implementing AI-driven predictive maintenance and intelligent process automation to enhance efficiency, reduce unplanned downtimes, and improve overall productivity.

The system integrates **IoT sensors, edge computing, and backend services** to monitor industrial machine performance in real-time. Sensors collect critical operational data including temperature, vibration, pressure, rotation speed, power consumption, and operational hours. This data is transmitted to the backend system via **MQTT protocol** for processing, analysis, and storage.

As a **Java Backend Developer**, the primary responsibility was to design and develop the **backend microservices** that receive, process, store, and analyze sensor data in real-time. The architecture followed a **microservices pattern** using Spring Boot, with separate services for data ingestion, processing, analytics, alerting, and reporting.

**Backend Architecture & Microservices:**

The system consisted of multiple microservices:

1. **Data Ingestion Service**: Receives real-time sensor data from industrial machines via MQTT broker (Eclipse Mosquitto). The service subscribes to MQTT topics, validates incoming sensor readings, and publishes validated data to **Apache Kafka** topics for downstream processing.

2. **Data Processing Service**: Consumes data from Kafka topics, performs data preprocessing including handling missing values using statistical methods (interpolation, NRSME), detecting outliers using Winsorization, and normalizing data for consistent analysis. Processed data is stored in **MySQL database** with proper indexing for time-series queries.

3. **Analytics Service**: Implements two-module pipeline for predictive maintenance:
   - **Static Module**: Applies rule-based thresholds and logistic regression models to identify machines showing early signs of deterioration
   - **Dynamic Module**: Uses machine learning algorithms (Random Forest, XGBoost) trained on historical sensor data to predict potential failures. Features include success rate trends, time-lag analysis, temperature variations, vibration patterns, and pressure anomalies.

4. **Alert Service**: Monitors analytics results and triggers real-time alerts when anomalies or predicted failures are detected. Sends notifications via email, SMS, and push notifications to maintenance teams. Maintains alert history and escalation rules.

5. **Dashboard API Service**: Provides **RESTful APIs** for the web-based dashboard, exposing endpoints for real-time monitoring, historical data visualization, machine health status, failure predictions, performance trends, and maintenance schedules.

**Database Design & Optimization:**

The **MySQL database** was designed with optimized schema for time-series data:

- **Machines Table**: Stores machine metadata including machine ID, model, location, installation date, specifications
- **Sensors Table**: Sensor metadata including sensor ID, type, machine association, calibration data
- **Sensor_Readings Table**: Time-series data with timestamp, sensor ID, reading value, quality indicators. Partitioned by date for query performance.
- **Predictions Table**: Stores ML model predictions including machine ID, prediction timestamp, failure probability, predicted failure date
- **Alerts Table**: Alert history with severity, type, machine, timestamp, acknowledgment status
- **Maintenance_Logs Table**: Maintenance activities, schedules, completion status

**Indexing strategies** implemented:

- Composite indexes on (machine_id, timestamp) for efficient time-range queries
- Covering indexes for frequently accessed columns
- Partitioning on sensor_readings table by month improving query performance by 50%

**Real-Time Data Processing:**

**Apache Kafka** served as the messaging backbone enabling:

- High-throughput data ingestion from multiple machines (5000+ sensor readings/second)
- Decoupling of services for independent scaling
- Data durability and replay capabilities
- Stream processing using Kafka Streams for real-time aggregations

**Redis** was used for:

- Caching latest sensor readings for instant dashboard updates
- Storing real-time machine status reducing database queries
- Session management for dashboard users
- Rate limiting for API endpoints

**RESTful APIs:**

Developed comprehensive REST APIs using **Spring Boot**:

- **Machine Management APIs**: CRUD operations for machine configuration
- **Real-Time Data APIs**: Get latest sensor readings, machine status, health score
- **Historical Data APIs**: Time-range queries with pagination, data export in CSV/Excel
- **Analytics APIs**: Failure predictions, trend analysis, comparative analytics
- **Alert APIs**: Manage alerts, acknowledgments, escalations
- **Maintenance APIs**: Schedule maintenance, log activities, track completion

API features included:

- JWT-based authentication for secure access
- Request validation using Bean Validation
- Comprehensive error handling with meaningful error codes
- API rate limiting preventing system overload
- API documentation using Swagger/OpenAPI

**Integration with IoT & Edge Devices:**

Integration with industrial IoT ecosystem:

- **AWS IoT Core** for device management and secure communication
- **MQTT broker** for lightweight, real-time sensor data transmission
- Message queuing ensuring no data loss during network interruptions
- Certificate-based device authentication

**Performance Optimization:**

System optimizations delivered significant improvements:

- **Data ingestion throughput**: Handles 5000+ sensor readings/second
- **API response time**: Average 80ms for real-time data endpoints
- **Database query optimization**: 50% improvement through partitioning and indexing
- **Cache hit ratio**: 85% for frequently accessed data using Redis
- **System uptime**: 99.8% availability through fault-tolerant architecture

**Monitoring & Analytics:**

- Real-time dashboard showing machine health, active alerts, performance KPIs
- Historical trend visualization for temperature, vibration, pressure metrics
- Predictive analytics dashboard with failure probability timelines
- Performance metrics tracking equipment failures reduced by 10-15%
- Maintenance cost reduction through proactive scheduling

**Development Practices:**

- **Agile/Scrum** with 2-week sprints
- **Code reviews** ensuring quality and knowledge sharing
- **Unit testing** with JUnit and Mockito achieving 80% coverage
- **Integration testing** for microservices communication
- **CI/CD pipeline** using Jenkins for automated deployments
- **Docker containerization** for consistent environments
- **Git** for version control with feature branch workflow

**Task Handled:**

- Designed and developed **microservices architecture** using Spring Boot for data ingestion, processing, analytics, and alerting
- Implemented **MQTT subscriber service** receiving real-time sensor data from industrial machines
- Integrated **Apache Kafka** for high-throughput message streaming and service decoupling
- Developed **data preprocessing logic** handling missing values, outliers, and normalization
- Designed **MySQL database schema** optimized for time-series sensor data with partitioning
- Implemented **indexing strategies** improving query performance by 50%
- Developed **RESTful APIs** for dashboard, real-time monitoring, historical data, and analytics
- Integrated **Redis caching** achieving 85% cache hit ratio for frequently accessed data
- Implemented **predictive analytics** using ML model integration for failure prediction
- Developed **alert service** with real-time notifications via email, SMS, and push notifications
- Implemented **JWT-based authentication** and authorization for secure API access
- Created **batch jobs** for data archival and cleanup using Spring Batch
- Developed **data export functionality** supporting CSV and Excel formats
- Implemented **API rate limiting** and request validation preventing system overload
- Integrated **AWS IoT Core** for secure device management and communication
- Configured **Kafka consumers** with proper offset management and error handling
- Developed **time-series data aggregation** APIs for trend analysis
- Implemented **database partitioning** on sensor_readings table for performance
- Created **comprehensive API documentation** using Swagger/OpenAPI
- Developed **unit tests** using JUnit and Mockito achieving 80% code coverage
- Configured **CI/CD pipeline** using Jenkins and Docker for automated deployments
- Implemented **logging** using Slf4j with structured logging for debugging
- Developed **health check endpoints** for service monitoring
- Optimized **database queries** using proper indexing and query analysis
- Implemented **connection pooling** using HikariCP for optimal performance
- Created **monitoring dashboards** tracking system metrics and performance KPIs

---

### **Project Sequence 3**

**Project Name:** Industrial Network Routers - OSS Provisioning & Activation
**Vertical:** Telecom OSS (Operations Support Systems) - Inventory Management
**Client:** Elisa Telecom, Finland
**Technology & Tool:** Java, Spring Boot, Spring Data JPA, Oracle Database, REST APIs, SOAP Web Services, Apache Camel, Maven, Git, JIRA
**Team Size:** 12
**Duration:** January 2024 - September 2024 (9 months)
**Role:** Java Backend Developer

**Detail Project Overview and Workflow:**

Elisa Telecom is a leading telecommunications provider in Finland offering mobile, fixed network, and digital services. The project focused on developing an **OSS (Operations Support Systems)** for provisioning and activating **Industrial 4G LTE Routers** used in enterprise networks. These high-performance routers support TD-LTE, FDD-LTE, TD-SCDMA, WCDMA, EVDO, CDMA1X, and GPRS/EDGE networks, designed for extreme temperature conditions (-35°C to +75°C) for unmanned systems requiring reliable communication.

The OSS system manages the complete lifecycle of network devices including **inventory management, service provisioning, activation, configuration, monitoring, and decommissioning**. The system integrates with multiple upstream systems (CRM, billing) and downstream systems (network elements, routers) to orchestrate end-to-end service delivery.

As a **Java Backend Developer**, the role involved developing backend services for device inventory management, provisioning workflows, activation processes, and integration with external systems using REST and SOAP web services.

**System Architecture & Components:**

The OSS platform consisted of multiple modules:

1. **Inventory Management Module**: Manages device inventory including routers, SIM cards, network elements. Tracks device lifecycle states (in-stock, allocated, activated, decommissioned). Implements inventory reservation and allocation logic.

2. **Provisioning Engine**: Orchestrates multi-step provisioning workflows including customer order validation, resource allocation, network configuration, and service activation. Implements workflow state machines with rollback capabilities for failures.

3. **Activation Service**: Handles device activation including SIM card association, network registration, configuration download, and initial connectivity testing. Communicates with network elements via SOAP web services.

4. **Configuration Management**: Manages router configurations including VPN settings, firewall rules, QoS policies, access control lists. Supports configuration templates and version control.

5. **Integration Layer**: Handles integration with external systems including CRM (customer data), billing systems (subscription management), and network management systems (device monitoring).

**Backend Development:**

Developed comprehensive **Spring Boot** applications with **layered architecture**:

**Inventory Management APIs:**

- Device CRUD operations (create, read, update, delete)
- Inventory search and filtering with advanced criteria
- Device allocation and reservation logic
- Stock level monitoring and replenishment alerts
- Bulk import of device inventory from Excel/CSV files

**Provisioning Workflow APIs:**

- Order creation and validation
- Multi-step provisioning orchestration using state machines
- Resource allocation (routers, SIM cards, IP addresses)
- Workflow status tracking and monitoring
- Rollback mechanisms for failed provisioning attempts

**Activation APIs:**

- Device activation with SIM card pairing
- Network registration and authentication
- Configuration download and application
- Connectivity testing and validation
- Activation status updates and notifications

**Database Design & Oracle Integration:**

**Oracle Database** served as the primary data store with comprehensive schema:

- **Devices Table**: Router inventory with serial numbers, models, hardware versions, current status
- **SIM_Cards Table**: SIM card inventory with ICCID, IMSI, network association, activation status
- **Orders Table**: Customer orders with order details, workflow state, timestamps
- **Provisioning_Tasks Table**: Individual workflow tasks with status, assigned resources, execution logs
- **Configurations Table**: Device configurations with version history
- **Integration_Logs Table**: External system integration audit trail

**Database features implemented:**

- **Stored procedures** for complex provisioning logic executed transactionally
- **Triggers** for audit logging and state validation
- **Views** for reporting and analytics joining multiple tables
- **Sequences** for generating unique identifiers
- **Partitioning** on large tables for performance

**PL/SQL development:**

- Complex stored procedures for provisioning workflows
- Functions for data validation and business rules
- Packages for modular code organization
- Exception handling ensuring data consistency

**Integration with External Systems:**

**Apache Camel** framework used for enterprise integration:

- **REST API integration** with CRM systems fetching customer data
- **SOAP web services** integration with network elements for device configuration
- **File-based integration** processing CSV/Excel inventory files
- **Message routing** with error handling and retry logic
- **Data transformation** between different system formats (XML, JSON, CSV)
- **JMS messaging** for asynchronous order processing

Integration patterns implemented:

- Content-based routing directing requests to appropriate systems
- Message transformation converting between data formats
- Error handling with dead letter queues
- Retry policies for transient failures
- Circuit breaker pattern preventing cascading failures

**RESTful APIs:**

Developed comprehensive REST API suite:

**Device Management:**

- GET /devices - List devices with pagination and filtering
- POST /devices - Add new device to inventory
- PUT /devices/{id} - Update device information
- DELETE /devices/{id} - Decommission device
- GET /devices/{id}/status - Get real-time device status

**Provisioning:**

- POST /orders - Create provisioning order
- GET /orders/{id} - Get order status and details
- PUT /orders/{id}/execute - Execute provisioning workflow
- POST /orders/{id}/rollback - Rollback failed provisioning
- GET /orders/{id}/logs - Get detailed execution logs

**Activation:**

- POST /activations - Initiate device activation
- GET /activations/{id}/status - Check activation progress
- POST /activations/{id}/test - Run connectivity tests
- PUT /activations/{id}/complete - Complete activation process

API features:

- OAuth 2.0 authentication for secure access
- Request/response validation using JSON schema
- Comprehensive error responses with error codes
- API versioning supporting backward compatibility
- Rate limiting preventing abuse
- HATEOAS links for resource navigation

**SOAP Web Services:**

Developed SOAP web services for network element integration:

- **Device Configuration Service**: Push configuration to routers
- **Status Query Service**: Poll device operational status
- **Network Registration Service**: Register device on cellular network
- **Diagnostic Service**: Run diagnostic tests on devices

SOAP features:

- WSDL-first development approach
- WS-Security for authentication
- SOAP fault handling with detailed error information
- Message logging for debugging and audit

**Performance & Scalability:**

Performance optimizations:

- **Database connection pooling** with optimal pool sizing
- **Query optimization** using explain plans reducing execution time by 45%
- **Caching** frequently accessed data (device models, configuration templates)
- **Batch processing** for bulk operations improving throughput
- **Asynchronous processing** for long-running provisioning workflows
- **Pagination** for API responses limiting data transfer

**Testing & Quality:**

- **Unit testing** with JUnit and Mockito achieving 75% code coverage
- **Integration testing** for database operations and external integrations
- **SOAP UI testing** for web service validation
- **Postman collections** for API testing
- **Load testing** using JMeter validating system performance
- **Mock services** for external system testing

**Development Practices:**

- **Agile/Scrum** methodology with 3-week sprints
- **JIRA** for task tracking and sprint management
- **Confluence** for technical documentation
- **Git** with GitFlow branching strategy
- **Code reviews** ensuring quality standards
- **Jenkins CI/CD** for automated builds and deployments
- **SonarQube** for code quality analysis

**Task Handled:**

- Designed and developed **inventory management system** using Spring Boot managing 100,000+ devices
- Implemented **provisioning workflow engine** with state machines and rollback capabilities
- Developed **device activation service** integrating with network elements via SOAP web services
- Designed **Oracle database schema** with stored procedures, triggers, and views
- Developed **PL/SQL packages** for complex provisioning logic executed transactionally
- Implemented **RESTful APIs** for device management, provisioning, and activation (50+ endpoints)
- Integrated **Apache Camel** for enterprise system integration with routing, transformation, error handling
- Developed **SOAP web services** for network element communication with WS-Security
- Implemented **OAuth 2.0 authentication** for secure API access
- Created **batch processing jobs** using Spring Batch for bulk device imports
- Developed **configuration management** system with version control and templates
- Implemented **data validation framework** ensuring data integrity across workflows
- Optimized **Oracle database queries** improving performance by 45% using explain plans
- Developed **integration layer** connecting CRM, billing, and network management systems
- Implemented **message routing** using Apache Camel with error handling and retry logic
- Created **stored procedures** in PL/SQL for complex business logic
- Developed **audit logging** mechanism tracking all system operations
- Implemented **asynchronous processing** for long-running provisioning tasks
- Created **comprehensive API documentation** using Swagger for REST APIs and WSDL for SOAP
- Developed **unit tests** using JUnit and Mockito achieving 75% code coverage
- Implemented **connection pooling** using HikariCP for optimal database performance
- Created **mock services** for testing external system integrations
- Developed **error handling framework** with proper exception hierarchy and error codes
- Implemented **transaction management** ensuring ACID properties for multi-step workflows
- Created **monitoring endpoints** exposing system health and performance metrics
- Developed **data export functionality** for inventory reports in Excel and CSV formats
- Configured **JMS messaging** for asynchronous order processing
- Implemented **circuit breaker pattern** preventing cascading failures in integrations
- Optimized **batch operations** processing 10,000+ records efficiently
- Created **technical documentation** for system architecture, APIs, and database schema

---

## **Personal Details**

**Father name:** [Your Father's Name]
**Mother name:** [Your Mother's Name]
**Current address:** [Your City]
**District:** [Your District]
**Pin code:** [Your Pincode]
**Marital status:** [Married/Unmarried]
**Language:** English, Hindi, [Other Languages]

**Declaration:** I hereby declare that the above-mentioned information is correct to the best of my knowledge.

**Date:** ---------- **Place:** -----------

**Regards**
**Sai**
